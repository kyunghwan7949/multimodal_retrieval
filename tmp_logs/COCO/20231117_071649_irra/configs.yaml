MLM: true
act_dropout: 0.0
activation_fn: gelu
alpha: 0.9
apply_graphormer_init: false
attention_dropout: 0.1
auto_resume: true
batch_size: 32
beta: 0.999
bias_lr_factor: 2.0
clip_grad: null
cmt_depth: 4
color_jitter: 0.0
data_path: /data/data2/IRRA/COCO/images/
dataset_name: COCO
device: cuda
dist_on_itp: false
dist_url: env://
distributed: false
drop_path: 0.0
dropout: 0.1
encoder_attention_heads: 16
encoder_embed_dim: 1024
encoder_ffn_embed_dim: 4096
encoder_layers: 12
encoder_normalize_before: true
eval_period: 1
gamma: 0.1
id_loss_weight: 1.0
imagenet_default_mean_and_std: true
img_aug: true
img_size: !!python/tuple
- 224
- 224
input_size: !!python/tuple
- '2'
- '2'
- '4'
lap_node_id: true
lap_node_id_eig_dropout: 0.2
lap_node_id_k: 16
lap_node_id_sign_flip: true
local_rank: 0
log_dir: null
log_period: 100
loss_names: mse
lr: 3.75e-05
lr_factor: 5.0
lrscheduler: cosine
mask_ratio: 0.75
masked_token_rate: 0.8
masked_token_unchanged_rate: 0.1
milestones: !!python/tuple
- 20
- 50
min_lr: 1.0e-05
mlm_loss_weight: 1.0
model: /data/data2/khahn/coco_irra_phrase_masking_2/mae/output/pretrain_mae_small_patch16_224
momentum: 0.9
name: irra
normlize_target: true
num_epoch: 70
num_instance: 4
num_workers: 8
opt_betas: null
opt_eps: 1.0e-08
optimizer: Adam
orf_node_id: false
orf_node_id_dim: 64
output_dir: tmp_logs/COCO/20231117_071649_irra
patch_size: 4
performer: false
performer_feature_redraw_interval: null
performer_finetune: false
performer_generalized_attention: false
performer_nb_features: null
pin_mem: true
postnorm: false
power: 0.9
prenorm: true
pretrain_choice: ViT-L/14
rand_node_id: false
rand_node_id_dim: 64
resume: false
resume_ckpt_file: ''
return_attention: false
root_dir: /data/data2/IRRA
sampler: random
save_ckpt_freq: 20
seed: 0
share_encoder_input_output_embed: false
start_epoch: 0
stochastic_depth: false
stride_size: 16
target_lr: 0
temperature: 0.02
test_batch_size: 256
text_length: 77
train_interpolation: bicubic
training: true
type_id: false
val_dataset: test
vocab_size: 28966
warmup_epochs: 5
warmup_factor: 0.1
warmup_lr: 1.0e-06
warmup_method: linear
warmup_steps: -1
weight_decay: 4.0e-05
weight_decay_bias: 0.0
world_size: 1
